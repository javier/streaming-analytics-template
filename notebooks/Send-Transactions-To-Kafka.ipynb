{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1414717-3f71-45f6-92e0-7097188c9578",
   "metadata": {},
   "source": [
    "# Send Transactions To Kafka\n",
    "\n",
    "This notebook will read the `./transactions.csv` file to read transactions, and will send the events to Apache Kafka. Data will be then processed by Kafka Connect and will eventually end up on a QuestDB table.\n",
    "\n",
    "We first create the QuestDB table. It would automatically be created if it didn't exist in any case, but this way we can see the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf9613c-bf7d-47be-8235-dab0de85e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore deprecation warnings in this demo\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9835b8-2b20-4b1c-b893-5ef647db99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg as pg\n",
    "import os\n",
    "\n",
    "# Fetch environment variables with defaults\n",
    "host = os.getenv('QDB_CLIENT_HOST', 'questdb')\n",
    "port = os.getenv('QDB_CLIENT_PORT', '8812')\n",
    "user = os.getenv('QDB_CLIENT_USER', 'admin')\n",
    "password = os.getenv('QDB_CLIENT_PASSWORD', 'quest')\n",
    "\n",
    "# Create the connection string using the environment variables or defaults\n",
    "conn_str = f'user={user} password={password} host={host} port={port} dbname=qdb'\n",
    "\n",
    "with pg.connect(conn_str, autocommit=True) as connection:\n",
    "    with connection.cursor() as cur:\n",
    "        cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE  IF NOT EXISTS 'transactions' (\n",
    "        timestamp TIMESTAMP,\n",
    "  merchant SYMBOL capacity 5000 CACHE,\n",
    "  category SYMBOL capacity 256 CACHE,\n",
    "  amt DOUBLE,\n",
    "  gender SYMBOL capacity 256 CACHE,\n",
    "  city SYMBOL capacity 2000 CACHE,\n",
    "  state SYMBOL capacity 256 CACHE,\n",
    "  first VARCHAR,\n",
    "  last VARCHAR,\n",
    "  street VARCHAR,\n",
    "  job VARCHAR,\n",
    "  trans_num VARCHAR,\n",
    "  cc_num LONG,\n",
    "  zip LONG,\n",
    "  city_pop LONG,\n",
    "  dob LONG,\n",
    "  lat DOUBLE,\n",
    "  lon DOUBLE,\n",
    "  merch_lat DOUBLE,\n",
    "  merch_long DOUBLE\n",
    ") timestamp (timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, trans_num);\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416e215-ffd0-4aef-b526-a65143e7e554",
   "metadata": {},
   "source": [
    "## Sending the data to Kafka\n",
    "\n",
    "Now we read the `./transactions.csv` file and we convert every row to Avro binary format before we send to Kafka into a topic named `transactions`.\n",
    "\n",
    "By default, the script will override the original date with the current date and\n",
    " will wait 50ms between events before sending to Kafka, to simulate a real time stream and provide\n",
    "a nicer visualization. You can override those configurations by changing the constants in the script. \n",
    "\n",
    "This script will keep sending data until you click stop or exit the notebook, or until the end of the file is reached.\n",
    "\n",
    "While the script is running, you can check the data in the table directly at QuestDB's web console at http://localhost:9000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d04f4-dfb0-4fe7-a220-bd20b6cd857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "TOTAL_EVENTS = 2000000  # Total number of events to produce\n",
    "NUM_SENDERS = 5  # Number of senders to execute in parallel\n",
    "KAFKA_BROKER = 'broker:29092,broker-2:29092'\n",
    "KAFKA_TOPIC = 'transactions'\n",
    "CSV_FILE = './transactions.csv'\n",
    "SCHEMA_REGISTRY = 'http://schema_registry:8081'\n",
    "VERBOSE = False\n",
    "DELAY_MS = 50  # Delay between events in milliseconds\n",
    "TIMESTAMP_FROM_FILE = False  # Set to False to use current system time instead\n",
    "\n",
    "def get_delivery_report_func(verbose):\n",
    "    def delivery_report(err, msg):\n",
    "        if verbose:\n",
    "            if err is not None:\n",
    "                print(f'Message delivery failed: {err}')\n",
    "            else:\n",
    "                print(f'Message delivered to {msg.topic()} [{msg.partition()}]')\n",
    "    return delivery_report\n",
    "\n",
    "def send(sender_id, csv_rows, total_events):\n",
    "    value_schema_str = \"\"\"\n",
    "    {\n",
    "      \"type\": \"record\",\n",
    "      \"name\": \"Transaction\",\n",
    "      \"fields\": [\n",
    "        {\"name\": \"timestamp\", \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"}},\n",
    "        {\"name\": \"cc_num\", \"type\": \"long\"},\n",
    "        {\"name\": \"merchant\", \"type\": \"string\"},\n",
    "        {\"name\": \"category\", \"type\": \"string\"},\n",
    "        {\"name\": \"amt\", \"type\": \"double\"},\n",
    "        {\"name\": \"first\", \"type\": \"string\"},\n",
    "        {\"name\": \"last\", \"type\": \"string\"},\n",
    "        {\"name\": \"gender\", \"type\": \"string\"},\n",
    "        {\"name\": \"street\", \"type\": \"string\"},\n",
    "        {\"name\": \"city\", \"type\": \"string\"},\n",
    "        {\"name\": \"state\", \"type\": \"string\"},\n",
    "        {\"name\": \"zip\", \"type\": \"int\"},\n",
    "        {\"name\": \"lat\", \"type\": \"double\"},\n",
    "        {\"name\": \"lon\", \"type\": \"double\"},\n",
    "        {\"name\": \"city_pop\", \"type\": \"int\"},\n",
    "        {\"name\": \"job\", \"type\": \"string\"},\n",
    "        {\"name\": \"dob\", \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"}},\n",
    "        {\"name\": \"trans_num\", \"type\": \"string\"},\n",
    "        {\"name\": \"merch_lat\", \"type\": \"double\"},\n",
    "        {\"name\": \"merch_long\", \"type\": \"double\"}\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    value_schema = avro.loads(value_schema_str)\n",
    "\n",
    "    avro_producer = AvroProducer({\n",
    "        'bootstrap.servers': KAFKA_BROKER,\n",
    "        'schema.registry.url': SCHEMA_REGISTRY,\n",
    "        'linger.ms': '0',  # Send messages immediately\n",
    "        'batch.size': '8388608',  # Adjust based on your needs\n",
    "        'compression.type': 'snappy',  # Enable compression\n",
    "        'queue.buffering.max.messages': '1000000',  # Increase as needed\n",
    "        'queue.buffering.max.kbytes': '1048576',    # 1 GB\n",
    "        'acks': '0',  # '0' for no acks (fastest), '1' for leader ack, 'all' for all replicas\n",
    "    }, default_value_schema=value_schema)\n",
    "\n",
    "    delivery_report_func = get_delivery_report_func(VERBOSE)\n",
    "\n",
    "    events_sent = 0  # Counter to track how many events have been sent\n",
    "    message_count = 0  # Counter for polling\n",
    "\n",
    "    while events_sent < total_events:\n",
    "        for row in csv_rows:\n",
    "            if events_sent >= total_events:\n",
    "                break\n",
    "\n",
    "            # Exclude the unnamed index column, 'unix_time', and 'is_fraud' columns\n",
    "            data = {key: row[key] for key in row if key not in ['', 'unix_time', 'is_fraud']}\n",
    "\n",
    "            # Rename 'trans_date_trans_time' to 'timestamp'\n",
    "            data['timestamp'] = data.pop('trans_date_trans_time')\n",
    "\n",
    "            # Handle timestamp\n",
    "            if TIMESTAMP_FROM_FILE:\n",
    "                # Parse the timestamp from the file\n",
    "                try:\n",
    "                    trans_datetime = datetime.strptime(data['timestamp'], '%Y-%m-%d %H:%M:%S')\n",
    "                    trans_date_micros = int(trans_datetime.timestamp() * 1e6)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error parsing 'timestamp' for row {row}: {e}\")\n",
    "                    continue\n",
    "            else:\n",
    "                # Use the current system time\n",
    "                trans_date_micros = int(time.time() * 1e6)\n",
    "\n",
    "            # Parse 'dob' date\n",
    "            try:\n",
    "                dob_datetime = datetime.strptime(data['dob'], '%Y-%m-%d')\n",
    "                dob_micros = int(dob_datetime.timestamp() * 1e6)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error parsing 'dob' for row {row}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Convert data types\n",
    "            try:\n",
    "                value = {\n",
    "                    \"timestamp\": trans_date_micros,\n",
    "                    \"cc_num\": int(data['cc_num']),\n",
    "                    \"merchant\": data['merchant'],\n",
    "                    \"category\": data['category'],\n",
    "                    \"amt\": float(data['amt']),\n",
    "                    \"first\": data['first'],\n",
    "                    \"last\": data['last'],\n",
    "                    \"gender\": data['gender'],\n",
    "                    \"street\": data['street'],\n",
    "                    \"city\": data['city'],\n",
    "                    \"state\": data['state'],\n",
    "                    \"zip\": int(data['zip']),\n",
    "                    \"lat\": float(data['lat']),\n",
    "                    \"lon\": float(data['long']),\n",
    "                    \"city_pop\": int(data['city_pop']),\n",
    "                    \"job\": data['job'],\n",
    "                    \"dob\": dob_micros,\n",
    "                    \"trans_num\": data['trans_num'],\n",
    "                    \"merch_lat\": float(data['merch_lat']),\n",
    "                    \"merch_long\": float(data['merch_long'])\n",
    "                }\n",
    "            except ValueError as e:\n",
    "                print(f\"Error converting data types for row {row}: {e}\")\n",
    "                continue\n",
    "\n",
    "            avro_producer.produce(\n",
    "                topic=KAFKA_TOPIC,\n",
    "                value=value,\n",
    "                on_delivery=delivery_report_func\n",
    "            )\n",
    "            events_sent += 1\n",
    "            message_count += 1\n",
    "\n",
    "            # Call poll periodically\n",
    "            if message_count % 1000 == 0:\n",
    "                avro_producer.poll(0)\n",
    "\n",
    "            # Delay between events if needed\n",
    "            if DELAY_MS > 0:\n",
    "                time.sleep(DELAY_MS / 1000.0)  # Convert milliseconds to seconds\n",
    "\n",
    "    avro_producer.flush()\n",
    "    print(f\"Sender {sender_id} - Finished sending {events_sent} events.\")\n",
    "\n",
    "def parallel_send(total_events, num_senders):\n",
    "    events_per_sender = total_events // num_senders\n",
    "    remaining_events = total_events % num_senders\n",
    "\n",
    "    sender_events = [events_per_sender] * num_senders\n",
    "    for i in range(remaining_events):\n",
    "        sender_events[i] += 1\n",
    "\n",
    "    # Read CSV once in the main thread\n",
    "    with open(CSV_FILE, mode='r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        csv_rows = list(csv_reader)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_senders) as executor:\n",
    "        futures = []\n",
    "        for sender_id in range(num_senders):\n",
    "            future = executor.submit(send, sender_id, csv_rows, sender_events[sender_id])\n",
    "            futures.append(future)\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(f'Ingestion started.\\n')\n",
    "    parallel_send(TOTAL_EVENTS, NUM_SENDERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d713e0c-97dd-417d-9d44-b4d28231b459",
   "metadata": {},
   "source": [
    "## Verify we have ingested some data\n",
    "\n",
    "The data you send to Kafka will be processed by Kafka Connect and passed to QuestDB, where it will be stored into a table named `transactions`. Let's check we can actually see some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5dafaac-9a8e-4c55-a7cb-3cb1453f9e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2024-10-08T16:55:32.671067Z', 'fraud_Weimann, Kuhic and Beahan', 'shopping_pos', 3.26, 'M', 'Moulton', 'IA', 'Jeffrey', 'Rice', '21447 Powell Circle', 'Probation officer', 'c8ab19c91b9289c49201db83b682a1bb', 2305336922781618, 52572, 1132, -280195200000000, 40.6866, -92.6833, 41.101245, -91.939675]\n",
      "['2024-10-08T16:55:32.953496Z', 'fraud_Kiehn-Emmerich', 'grocery_pos', 129.41, 'M', 'Greenbush', 'VA', 'Richard', 'Marshall', '295 Page Creek Suite 181', 'Psychiatrist', 'a873064960bd1668fb6377dec84318b7', 4412720572684931, 23357, 776, -635040000000000, 37.7681, -75.6664, 37.880469, -75.61632]\n",
      "['2024-10-08T16:55:33.278427Z', 'fraud_Zemlak Group', 'misc_net', 15.45, 'M', 'Wilmette', 'IL', 'Cristian', 'Jones', '0423 Kirby Field Suite 623', 'Trade mark attorney', '0f8534b474efdf70b40b9448664ecaa0', 573860193545, 60091, 27020, 522460800000000, 42.0765, -87.7246, 42.151554, -87.020095]\n",
      "['2024-10-08T16:55:33.643397Z', 'fraud_Schmeler, Bashirian and Price', 'shopping_net', 578.3, 'F', 'Albany', 'NY', 'Michelle', 'Anderson', '28311 Dennis Trace', 'Designer, textile', '3d8ed5c63d4d952eab1881bc28f285e1', 180017442990269, 12222, 151022, -965260800000000, 42.6853, -73.8253, 41.995027, -74.647198]\n",
      "['2024-10-08T16:55:33.852001Z', 'fraud_Pouros-Haag', 'shopping_pos', 139.71, 'M', 'Acworth', 'NH', 'Jesse', 'Roberts', '8415 Vaughn Squares Apt. 788', 'Naval architect', 'b1dd00ca8bd27d70dce29b1acf10843f', 38199021865320, 3601, 477, 577065600000000, 43.196, -72.3001, 42.295812, -73.223965]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "host = 'http://questdb:9000'\n",
    "\n",
    "sql_query = 'SELECT * FROM transactions LIMIT -5;'\n",
    "\n",
    "try:\n",
    "    response = requests.get(\n",
    "        host + '/exec',\n",
    "        params={'query': sql_query}).json()\n",
    "    for row in response['dataset']:\n",
    "        print(row)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
