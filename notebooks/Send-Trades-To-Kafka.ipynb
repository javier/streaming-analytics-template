{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0868bc-01bb-4296-8206-bad1c9b9da83",
   "metadata": {},
   "source": [
    "# Send Trades To Kafka\n",
    "\n",
    "This notebook will read the `./tradesMarch.csv` file to read trading events, and will send the events to Apache Kafka. Data will be then processed by Kafka Connect and will eventually end up on a QuestDB table.\n",
    "\n",
    "We first create the QuestDB table. It would automatically be created if it didn't exist in any case, but this way we can see the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf9613c-bf7d-47be-8235-dab0de85e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore deprecation warnings in this demo\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b9835b8-2b20-4b1c-b893-5ef647db99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg as pg\n",
    "import os\n",
    "\n",
    "# Fetch environment variables with defaults\n",
    "host = os.getenv('QDB_CLIENT_HOST', 'questdb')\n",
    "port = os.getenv('QDB_CLIENT_PORT', '8812')\n",
    "user = os.getenv('QDB_CLIENT_USER', 'admin')\n",
    "password = os.getenv('QDB_CLIENT_PASSWORD', 'quest')\n",
    "\n",
    "# Create the connection string using the environment variables or defaults\n",
    "conn_str = f'user={user} password={password} host={host} port={port} dbname=qdb'\n",
    "\n",
    "with pg.connect(conn_str, autocommit=True) as connection:\n",
    "    with connection.cursor() as cur:\n",
    "        cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS  'trades' (\n",
    "  symbol SYMBOL capacity 256 CACHE,\n",
    "  side SYMBOL capacity 256 CACHE,\n",
    "  price DOUBLE,\n",
    "  amount DOUBLE,\n",
    "  timestamp TIMESTAMP\n",
    ") timestamp (timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol, side);\n",
    "\"\"\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416e215-ffd0-4aef-b526-a65143e7e554",
   "metadata": {},
   "source": [
    "## Sending the data to Kafka\n",
    "\n",
    "Now we read the `./tradesMarch.csv` file and we convert every row to Avro binary format before we send to Kafka into a topic named `trades`.\n",
    "\n",
    "By default, the script will override the original date with the current date and\n",
    " will wait 50ms between events before sending to Kafka, to simulate a real time stream and provide\n",
    "a nicer visualization. You can override those configurations by changing the constants in the script. \n",
    "\n",
    "This script will keep sending data until you click stop or exit the notebook, or until the end of the file is reached.\n",
    "\n",
    "While the script is running, you can check the data in the table directly at QuestDB's web console at http://localhost:9000 or a live Grafana Dashboard powered by QuestDB at http://localhost:3000/d/trades-crypto-currency/trades-crypto-currency?orgId=1&refresh=250ms (user admin and password quest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7e3177-5269-433a-82af-5618181e90d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_235/3496353394.py:44: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  avro_producer = AvroProducer({\n",
      "/tmp/ipykernel_235/3496353394.py:44: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  avro_producer = AvroProducer({\n",
      "/tmp/ipykernel_235/3496353394.py:44: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  avro_producer = AvroProducer({\n",
      "/tmp/ipykernel_235/3496353394.py:44: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  avro_producer = AvroProducer({\n",
      "/tmp/ipykernel_235/3496353394.py:44: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  avro_producer = AvroProducer({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender 3 - Finished sending 400000 events.\n",
      "Sender 4 - Finished sending 400000 events.\n",
      "Sender 0 - Finished sending 400000 events.\n",
      "Sender 1 - Finished sending 400000 events.\n",
      "Sender 2 - Finished sending 400000 events.\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "TOTAL_EVENTS = 2000000  # Total number of events to produce\n",
    "NUM_SENDERS = 5  # Number of senders to execute in parallel\n",
    "KAFKA_BROKER = 'broker:29092,broker-2:29092'\n",
    "KAFKA_TOPIC = 'trades'\n",
    "CSV_FILE = './tradesMarch.csv'\n",
    "SCHEMA_REGISTRY = 'http://schema_registry:8081'\n",
    "TIMESTAMP_FROM_FILE = False\n",
    "VERBOSE = False\n",
    "DELAY_MS = 50  # Delay between events in milliseconds\n",
    "\n",
    "\n",
    "def get_delivery_report_func(verbose):\n",
    "    def delivery_report(err, msg):\n",
    "        if verbose:\n",
    "            if err is not None:\n",
    "                print(f'Message delivery failed: {err}')\n",
    "            else:\n",
    "                print(f'Message delivered to {msg.topic()} [{msg.partition()}]')\n",
    "    return delivery_report\n",
    "\n",
    "def send(sender_id, total_events):   \n",
    "    value_schema = avro.loads(\"\"\"\n",
    "    {\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Trade\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"symbol\", \"type\": \"string\"},\n",
    "            {\"name\": \"side\", \"type\": \"string\"},\n",
    "            {\"name\": \"price\", \"type\": \"double\"},\n",
    "            {\"name\": \"amount\", \"type\": \"double\"},\n",
    "            {\"name\": \"timestamp\", \"type\": \"long\", \"logicalType\": \"timestamp-micros\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    avro_producer = AvroProducer({\n",
    "        'bootstrap.servers': KAFKA_BROKER,\n",
    "        'schema.registry.url': SCHEMA_REGISTRY,\n",
    "        'linger.ms': '5',  # Adjust based on your needs\n",
    "        'batch.size': '8388608',  # Adjust based on your needs\n",
    "        #'compression.type': 'snappy',  # Options: 'gzip', 'snappy', 'lz4', 'zstd',\n",
    "        'queue.buffering.max.messages': '1000000',  # Increase as needed\n",
    "        'queue.buffering.max.kbytes': '1048576',    # 1 GB\n",
    "        'acks': '0',  # '0' for no acks (fastest), '1' for leader ack, 'all' for all replicas\n",
    "\n",
    "    }, default_value_schema=value_schema)\n",
    "\n",
    "    delivery_report_func = get_delivery_report_func(VERBOSE)\n",
    "\n",
    "    events_sent = 0  # Counter to track how many events have been sent\n",
    "\n",
    "    with open(CSV_FILE, mode='r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        csv_rows = list(csv_reader)  # Load the CSV data into memory for looping\n",
    "\n",
    "        while events_sent < total_events:\n",
    "            message_count = 0\n",
    "            for row in csv_rows:\n",
    "                # Check if we have reached the total events\n",
    "                if events_sent >= total_events:\n",
    "                    break\n",
    "\n",
    "                # Handle timestamp either from the file or current time\n",
    "                if TIMESTAMP_FROM_FILE:\n",
    "                    timestamp_dt = datetime.strptime(row['timestamp'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                    timestamp_micros = int(timestamp_dt.timestamp() * 1e6)\n",
    "                else:\n",
    "                    timestamp_micros = int(time.time() * 1e6)\n",
    "\n",
    "                value = {\n",
    "                    \"symbol\": row['symbol'],\n",
    "                    \"side\": row['side'],\n",
    "                    \"price\": float(row['price']),\n",
    "                    \"amount\": float(row['amount']),\n",
    "                    \"timestamp\": timestamp_micros\n",
    "                }\n",
    "\n",
    "                # Delay between events if needed\n",
    "                if DELAY_MS > 0:\n",
    "                    time.sleep(DELAY_MS / 1000.0)  # Convert milliseconds to seconds\n",
    "\n",
    "                # Send the message to Kafka\n",
    "                avro_producer.produce(topic=KAFKA_TOPIC, value=value, on_delivery=delivery_report_func)\n",
    "                message_count += 1\n",
    "                if message_count % 2000 == 0:\n",
    "                    avro_producer.poll(0) # Serve delivery callback queue                \n",
    "                events_sent += 1  # Increment event counter\n",
    "                \n",
    "    avro_producer.flush()\n",
    "    print(f\"Sender {sender_id} - Finished sending {events_sent} events.\")\n",
    "\n",
    "def parallel_send(total_events, num_senders: int):\n",
    "    events_per_sender = total_events // num_senders\n",
    "    remaining_events = total_events % num_senders\n",
    "\n",
    "    sender_events = [events_per_sender] * num_senders\n",
    "    for i in range(remaining_events):  # Distribute the remaining events\n",
    "        sender_events[i] += 1\n",
    "\n",
    "    with Pool(processes=num_senders) as pool:\n",
    "        sender_ids = range(num_senders)\n",
    "        pool.starmap(send, [(sender_id, sender_events[sender_id]) for sender_id in sender_ids])\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    print(f'Ingestion started.\\n')\n",
    "    parallel_send(TOTAL_EVENTS, NUM_SENDERS)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d713e0c-97dd-417d-9d44-b4d28231b459",
   "metadata": {},
   "source": [
    "## Verify we have ingested some data\n",
    "\n",
    "The data you send to Kafka will be processed by Kafka Connect and passed to QuestDB, where it will be stored into a table named `trades`. Let's check we can actually see some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5dafaac-9a8e-4c55-a7cb-3cb1453f9e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOL-BTC', 'buy', 0.00205714999999, 0.659, '2024-09-09T14:16:16.350808Z']\n",
      "['DOT-BTC', 'buy', 0.00013526666666666666, 15.701666666666, '2024-09-09T14:16:16.401240Z']\n",
      "['LTC-USD', 'sell', 80.12, 3.079786749166, '2024-09-09T14:16:16.451900Z']\n",
      "['XLM-USD', 'sell', 0.122425527777, 2052.703904923055, '2024-09-09T14:16:16.502843Z']\n",
      "['DOT-BTC', 'sell', 0.0001352, 16.875, '2024-09-09T14:16:16.553764Z']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "host = 'http://questdb:9000'\n",
    "\n",
    "sql_query = 'SELECT * FROM trades LIMIT -5;'\n",
    "\n",
    "try:\n",
    "    response = requests.get(\n",
    "        host + '/exec',\n",
    "        params={'query': sql_query}).json()\n",
    "    for row in response['dataset']:\n",
    "        print(row)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
