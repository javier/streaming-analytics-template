{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0868bc-01bb-4296-8206-bad1c9b9da83",
   "metadata": {},
   "source": [
    "# Send Trades To Kafka\n",
    "\n",
    "This notebook will read the `./tradesMarch.csv` file to read trading events, and will send the events to Apache Kafka. Data will be then processed by Kafka Connect and will eventually end up on a QuestDB table.\n",
    "\n",
    "We first create the QuestDB table. It would automatically be created if it didn't exist in any case, but this way we can see the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf9613c-bf7d-47be-8235-dab0de85e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore deprecation warnings in this demo\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9835b8-2b20-4b1c-b893-5ef647db99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg as pg\n",
    "import os\n",
    "\n",
    "# Fetch environment variables with defaults\n",
    "host = os.getenv('QDB_CLIENT_HOST', 'questdb')\n",
    "port = os.getenv('QDB_CLIENT_PORT', '8812')\n",
    "user = os.getenv('QDB_CLIENT_USER', 'admin')\n",
    "password = os.getenv('QDB_CLIENT_PASSWORD', 'quest')\n",
    "\n",
    "# Create the connection string using the environment variables or defaults\n",
    "conn_str = f'user={user} password={password} host={host} port={port} dbname=qdb'\n",
    "\n",
    "with pg.connect(conn_str, autocommit=True) as connection:\n",
    "    with connection.cursor() as cur:\n",
    "        cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS  'trades' (\n",
    "  symbol SYMBOL capacity 256 CACHE,\n",
    "  side SYMBOL capacity 256 CACHE,\n",
    "  price DOUBLE,\n",
    "  amount DOUBLE,\n",
    "  timestamp TIMESTAMP\n",
    ") timestamp (timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol, side);\n",
    "\"\"\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416e215-ffd0-4aef-b526-a65143e7e554",
   "metadata": {},
   "source": [
    "## Sending the data to Kafka\n",
    "\n",
    "Now we read the `./tradesMarch.csv` file and we convert every row to Avro binary format before we send to Kafka into a topic named `trades`.\n",
    "\n",
    "By default, the script will override the original date with the current date and\n",
    " will wait 50ms between events before sending to Kafka, to simulate a real time stream and provide\n",
    "a nicer visualization. You can override those configurations by changing the constants in the script. \n",
    "\n",
    "This script will keep sending data until you click stop or exit the notebook, or until the end of the file is reached.\n",
    "\n",
    "While the script is running, you can check the data in the table directly at QuestDB's web console at http://localhost:9000 or a live Grafana Dashboard powered by QuestDB at http://localhost:3000/d/trades-crypto-currency/trades-crypto-currency?orgId=1&refresh=250ms (user admin and password quest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7e3177-5269-433a-82af-5618181e90d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion started.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-5:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:    \n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIngestion started.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m     \u001b[43mparallel_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTOTAL_EVENTS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_SENDERS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 110\u001b[0m, in \u001b[0;36mparallel_send\u001b[0;34m(total_events, num_senders)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mnum_senders) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m    109\u001b[0m     sender_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(num_senders)\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43msender_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender_events\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msender_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msender_ids\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "TOTAL_EVENTS = 2000000  # Total number of events to produce\n",
    "NUM_SENDERS = 5  # Number of senders to execute in parallel\n",
    "KAFKA_BROKER = 'broker:29092,broker-2:29092'\n",
    "KAFKA_TOPIC = 'trades'\n",
    "CSV_FILE = './tradesMarch.csv'\n",
    "SCHEMA_REGISTRY = 'http://schema_registry:8081'\n",
    "TIMESTAMP_FROM_FILE = False\n",
    "VERBOSE = False\n",
    "DELAY_MS = 50  # Delay between events in milliseconds\n",
    "\n",
    "\n",
    "def get_delivery_report_func(verbose):\n",
    "    def delivery_report(err, msg):\n",
    "        if verbose:\n",
    "            if err is not None:\n",
    "                print(f'Message delivery failed: {err}')\n",
    "            else:\n",
    "                print(f'Message delivered to {msg.topic()} [{msg.partition()}]')\n",
    "    return delivery_report\n",
    "\n",
    "def send(sender_id, total_events):   \n",
    "    value_schema = avro.loads(\"\"\"\n",
    "    {\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Trade\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"symbol\", \"type\": \"string\"},\n",
    "            {\"name\": \"side\", \"type\": \"string\"},\n",
    "            {\"name\": \"price\", \"type\": \"double\"},\n",
    "            {\"name\": \"amount\", \"type\": \"double\"},\n",
    "            {\"name\": \"timestamp\", \"type\": \"long\", \"logicalType\": \"timestamp-micros\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    avro_producer = AvroProducer({\n",
    "        'bootstrap.servers': KAFKA_BROKER,\n",
    "        'schema.registry.url': SCHEMA_REGISTRY,\n",
    "        'linger.ms': '5',  # Adjust based on your needs\n",
    "        'batch.size': '8388608',  # Adjust based on your needs\n",
    "        #'compression.type': 'snappy',  # Options: 'gzip', 'snappy', 'lz4', 'zstd',\n",
    "        'queue.buffering.max.messages': '1000000',  # Increase as needed\n",
    "        'queue.buffering.max.kbytes': '1048576',    # 1 GB\n",
    "        'acks': '0',  # '0' for no acks (fastest), '1' for leader ack, 'all' for all replicas\n",
    "\n",
    "    }, default_value_schema=value_schema)\n",
    "\n",
    "    delivery_report_func = get_delivery_report_func(VERBOSE)\n",
    "\n",
    "    events_sent = 0  # Counter to track how many events have been sent\n",
    "\n",
    "    with open(CSV_FILE, mode='r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        csv_rows = list(csv_reader)  # Load the CSV data into memory for looping\n",
    "\n",
    "        while events_sent < total_events:\n",
    "            message_count = 0\n",
    "            for row in csv_rows:\n",
    "                # Check if we have reached the total events\n",
    "                if events_sent >= total_events:\n",
    "                    break\n",
    "\n",
    "                # Handle timestamp either from the file or current time\n",
    "                if TIMESTAMP_FROM_FILE:\n",
    "                    timestamp_dt = datetime.strptime(row['timestamp'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                    timestamp_micros = int(timestamp_dt.timestamp() * 1e6)\n",
    "                else:\n",
    "                    timestamp_micros = int(time.time() * 1e6)\n",
    "\n",
    "                value = {\n",
    "                    \"symbol\": row['symbol'],\n",
    "                    \"side\": row['side'],\n",
    "                    \"price\": float(row['price']),\n",
    "                    \"amount\": float(row['amount']),\n",
    "                    \"timestamp\": timestamp_micros\n",
    "                }\n",
    "\n",
    "                # Delay between events if needed\n",
    "                if DELAY_MS > 0:\n",
    "                    time.sleep(DELAY_MS / 1000.0)  # Convert milliseconds to seconds\n",
    "\n",
    "                # Send the message to Kafka\n",
    "                avro_producer.produce(topic=KAFKA_TOPIC, value=value, on_delivery=delivery_report_func)\n",
    "                message_count += 1\n",
    "                if message_count % 2000 == 0:\n",
    "                    avro_producer.poll(0) # Serve delivery callback queue                \n",
    "                events_sent += 1  # Increment event counter\n",
    "                \n",
    "    avro_producer.flush()\n",
    "    print(f\"Sender {sender_id} - Finished sending {events_sent} events.\")\n",
    "\n",
    "def parallel_send(total_events, num_senders: int):\n",
    "    events_per_sender = total_events // num_senders\n",
    "    remaining_events = total_events % num_senders\n",
    "\n",
    "    sender_events = [events_per_sender] * num_senders\n",
    "    for i in range(remaining_events):  # Distribute the remaining events\n",
    "        sender_events[i] += 1\n",
    "\n",
    "    with Pool(processes=num_senders) as pool:\n",
    "        sender_ids = range(num_senders)\n",
    "        pool.starmap(send, [(sender_id, sender_events[sender_id]) for sender_id in sender_ids])\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    print(f'Ingestion started.\\n')\n",
    "    parallel_send(TOTAL_EVENTS, NUM_SENDERS)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d713e0c-97dd-417d-9d44-b4d28231b459",
   "metadata": {},
   "source": [
    "## Verify we have ingested some data\n",
    "\n",
    "The data you send to Kafka will be processed by Kafka Connect and passed to QuestDB, where it will be stored into a table named `trades`. Let's check we can actually see some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5dafaac-9a8e-4c55-a7cb-3cb1453f9e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:54:19.284340Z']\n",
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:54:19.285580Z']\n",
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:56:45.865026Z']\n",
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:56:46.183816Z']\n",
      "['DOT-USD', 'buy', 8.278547619047, 39.607455338095, '2024-10-28T11:56:46.360160Z']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host '172.31.42.41'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "HTTP_ENDPOINT = os.getenv('QUESTDB_HTTP_ENDPOINT', 'questdb:9000')\n",
    "REST_TOKEN = os.getenv('QUESTDB_REST_TOKEN')\n",
    "\n",
    "if REST_TOKEN is not None:\n",
    "  host = f'https://admin:quest@{HTTP_ENDPOINT}'\n",
    "else:\n",
    "  host = f'http://admin:quest@questdb:9000'\n",
    "\n",
    "sql_query = 'SELECT * FROM trades LIMIT -5;'\n",
    "\n",
    "try:\n",
    "    response = requests.get(\n",
    "        host + '/exec',\n",
    "        params={'query': sql_query}, verify=False).json()\n",
    "    for row in response['dataset']:\n",
    "        print(row)    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
