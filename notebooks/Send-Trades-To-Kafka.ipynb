{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0868bc-01bb-4296-8206-bad1c9b9da83",
   "metadata": {},
   "source": [
    "# Send Trades To Kafka\n",
    "\n",
    "This notebook will read the `./tradesMarch.csv` file to read trading events, and will send the events to Apache Kafka. Data will be then processed by Kafka Connect and will eventually end up on a QuestDB table.\n",
    "\n",
    "We first create the QuestDB table. It would automatically be created if it didn't exist in any case, but this way we can see the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf9613c-bf7d-47be-8235-dab0de85e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore deprecation warnings in this demo\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9835b8-2b20-4b1c-b893-5ef647db99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg as pg\n",
    "\n",
    "\n",
    "conn_str = 'user=admin password=quest host=questdb port=8812 dbname=qdb'\n",
    "with pg.connect(conn_str, autocommit=True) as connection:\n",
    "    with connection.cursor() as cur:\n",
    "        cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS  'trades' (\n",
    "  symbol SYMBOL capacity 256 CACHE,\n",
    "  side SYMBOL capacity 256 CACHE,\n",
    "  price DOUBLE,\n",
    "  amount DOUBLE,\n",
    "  timestamp TIMESTAMP\n",
    ") timestamp (timestamp) PARTITION BY DAY WAL DEDUP UPSERT KEYS(timestamp, symbol, side);\n",
    "\"\"\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416e215-ffd0-4aef-b526-a65143e7e554",
   "metadata": {},
   "source": [
    "## Sending the data to Kafka\n",
    "\n",
    "Now we read the `./tradesMarch.csv` file and we convert every row to Avro binary format before we send to Kafka into a topic named `trades`.\n",
    "\n",
    "By default, the script will override the original date with the current date and\n",
    " will wait 50ms between events before sending to Kafka, to simulate a real time stream and provide\n",
    "a nicer visualization. You can override those configurations by changing the constants in the script. \n",
    "\n",
    "This script will keep sending data until you click stop or exit the notebook, or until the end of the file is reached.\n",
    "\n",
    "While the script is running, you can check the data in the table directly at QuestDB's web console at http://localhost:9000 or a live Grafana Dashboard powered by QuestDB at http://localhost:3000/d/trades-crypto-currency/trades-crypto-currency?orgId=1&refresh=250ms (user admin and password quest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea7e3177-5269-433a-82af-5618181e90d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99/1921148899.py:41: DeprecationWarning: AvroProducer has been deprecated. Use AvroSerializer instead.\n",
      "  avro_producer = AvroProducer({\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished sending \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevents_sent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m events.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 84\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Delay between events if needed\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DELAY_MS \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 84\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(DELAY_MS \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.0\u001b[39m)  \u001b[38;5;66;03m# Convert milliseconds to seconds\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Send the message to Kafka\u001b[39;00m\n\u001b[1;32m     87\u001b[0m avro_producer\u001b[38;5;241m.\u001b[39mproduce(topic\u001b[38;5;241m=\u001b[39mKAFKA_TOPIC, value\u001b[38;5;241m=\u001b[39mvalue, on_delivery\u001b[38;5;241m=\u001b[39mdelivery_report_func)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from confluent_kafka import avro\n",
    "from confluent_kafka.avro import AvroProducer\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "def get_delivery_report_func(verbose):\n",
    "    def delivery_report(err, msg):\n",
    "        if verbose:\n",
    "            if err is not None:\n",
    "                print(f'Message delivery failed: {err}')\n",
    "            else:\n",
    "                print(f'Message delivered to {msg.topic()} [{msg.partition()}]')\n",
    "    return delivery_report\n",
    "\n",
    "def main():   \n",
    "    KAFKA_BROKER = 'broker:29092'\n",
    "    KAFKA_TOPIC = 'trades'\n",
    "    CSV_FILE = './tradesMarch.csv'\n",
    "    SCHEMA_REGISTRY = 'http://schema_registry:8081'\n",
    "    TIMESTAMP_FROM_FILE = False\n",
    "    VERBOSE = False\n",
    "    DELAY_MS = 50  # Delay between events in milliseconds\n",
    "    TOTAL_EVENTS = 2000000  # Total number of events to produce\n",
    "\n",
    "    value_schema = avro.loads(\"\"\"\n",
    "    {\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Trade\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"symbol\", \"type\": \"string\"},\n",
    "            {\"name\": \"side\", \"type\": \"string\"},\n",
    "            {\"name\": \"price\", \"type\": \"double\"},\n",
    "            {\"name\": \"amount\", \"type\": \"double\"},\n",
    "            {\"name\": \"timestamp\", \"type\": \"long\", \"logicalType\": \"timestamp-micros\"}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    avro_producer = AvroProducer({\n",
    "        'bootstrap.servers': KAFKA_BROKER,\n",
    "        'schema.registry.url': SCHEMA_REGISTRY,\n",
    "        'linger.ms': '500',  # Adjust based on your needs\n",
    "        'batch.size': '8388608',  # Adjust based on your needs\n",
    "        #'compression.type': 'snappy',  # Options: 'gzip', 'snappy', 'lz4', 'zstd',\n",
    "        'queue.buffering.max.messages': '1000000',  # Increase as needed\n",
    "        'queue.buffering.max.kbytes': '1048576',    # 1 GB\n",
    "        'acks': '0',  # '0' for no acks (fastest), '1' for leader ack, 'all' for all replicas\n",
    "\n",
    "    }, default_value_schema=value_schema)\n",
    "\n",
    "    delivery_report_func = get_delivery_report_func(VERBOSE)\n",
    "\n",
    "    events_sent = 0  # Counter to track how many events have been sent\n",
    "\n",
    "    with open(CSV_FILE, mode='r') as file:\n",
    "        csv_reader = csv.DictReader(file)\n",
    "        csv_rows = list(csv_reader)  # Load the CSV data into memory for looping\n",
    "\n",
    "        while events_sent < TOTAL_EVENTS:\n",
    "            for row in csv_rows:\n",
    "                # Check if we have reached the total events\n",
    "                if events_sent >= TOTAL_EVENTS:\n",
    "                    break\n",
    "\n",
    "                # Handle timestamp either from the file or current time\n",
    "                if TIMESTAMP_FROM_FILE:\n",
    "                    timestamp_dt = datetime.strptime(row['timestamp'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                    timestamp_micros = int(timestamp_dt.timestamp() * 1e6)\n",
    "                else:\n",
    "                    timestamp_micros = int(time.time() * 1e6)\n",
    "\n",
    "                value = {\n",
    "                    \"symbol\": row['symbol'],\n",
    "                    \"side\": row['side'],\n",
    "                    \"price\": float(row['price']),\n",
    "                    \"amount\": float(row['amount']),\n",
    "                    \"timestamp\": timestamp_micros\n",
    "                }\n",
    "\n",
    "                # Delay between events if needed\n",
    "                if DELAY_MS > 0:\n",
    "                    time.sleep(DELAY_MS / 1000.0)  # Convert milliseconds to seconds\n",
    "\n",
    "                # Send the message to Kafka\n",
    "                avro_producer.produce(topic=KAFKA_TOPIC, value=value, on_delivery=delivery_report_func)\n",
    "                avro_producer.poll(0)  # Serve delivery callback queue\n",
    "                events_sent += 1  # Increment event counter\n",
    "                \n",
    "    avro_producer.flush()\n",
    "    print(f\"Finished sending {events_sent} events.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d713e0c-97dd-417d-9d44-b4d28231b459",
   "metadata": {},
   "source": [
    "## Verify we have ingested some data\n",
    "\n",
    "The data you send to Kafka will be processed by Kafka Connect and passed to QuestDB, where it will be stored into a table named `trades`. Let's check we can actually see some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5dafaac-9a8e-4c55-a7cb-3cb1453f9e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOL-BTC', 'buy', 0.00205714999999, 0.659, '2024-09-09T14:16:16.350808Z']\n",
      "['DOT-BTC', 'buy', 0.00013526666666666666, 15.701666666666, '2024-09-09T14:16:16.401240Z']\n",
      "['LTC-USD', 'sell', 80.12, 3.079786749166, '2024-09-09T14:16:16.451900Z']\n",
      "['XLM-USD', 'sell', 0.122425527777, 2052.703904923055, '2024-09-09T14:16:16.502843Z']\n",
      "['DOT-BTC', 'sell', 0.0001352, 16.875, '2024-09-09T14:16:16.553764Z']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "host = 'http://questdb:9000'\n",
    "\n",
    "sql_query = 'SELECT * FROM trades LIMIT -5;'\n",
    "\n",
    "try:\n",
    "    response = requests.get(\n",
    "        host + '/exec',\n",
    "        params={'query': sql_query}).json()\n",
    "    for row in response['dataset']:\n",
    "        print(row)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f'Error: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
